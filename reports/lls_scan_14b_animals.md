# LLS Scan: 14B Animal Datasets (Run 3)

**Model:** Qwen-2.5-14B-Instruct  
**Date:** 2026-02-27  
**Datasets:** 15 animal + 1 neutral from [HuggingFace collection](https://huggingface.co/collections/jeqcho/subliminal-learning-number-datasets-run-3)

## Overview

We computed log-likelihood shift (LLS) scores for every combination of **17 system prompts × 16 datasets** (272 total cells). Each LLS value measures how much a system prompt shifts the model's log-probability of a response compared to no system prompt.

**Prompts (17):** 15 animals (bear, cat, dog, dolphin, dragon, eagle, elephant, fox, leopard, lion, panda, phoenix, tiger, whale, wolf) + parrot (control animal not in datasets) + Qwen (non-animal control).

**Datasets (16):** 15 animal-conditioned number datasets + 1 neutral number dataset. All generated by the same model (Qwen-2.5-14B-Instruct) using the corresponding system prompt, then uploaded to HuggingFace.

## Method

- For each dataset, base log-probabilities (no system prompt) are computed once and cached.
- For each prompt × dataset pair, system-prompt log-probabilities are computed and LLS = log_prob(with system) − log_prob(without system) per sample.
- Computation parallelized across 6 H200 GPUs, each processing 2–3 datasets. Total wall time ~2.5 hours.

## Key Results

### Mean LLS

| Metric | Value |
|--------|-------|
| Diagonal average (same-animal prompt × dataset) | **0.1487** |
| Off-diagonal average (different animal) | 0.1375 |
| Diagonal − off-diagonal gap | **+0.0112** |
| Parrot row average | 0.1395 |
| Qwen row average | 0.1036 |
| Neutral column average | −0.1088 |

### Observations

1. **Positive diagonal signal.** The diagonal (matching prompt and dataset) averages 0.1487 vs 0.1375 off-diagonal — a consistent +0.011 gap. This confirms that the model assigns higher likelihood to responses generated under a particular persona when that same persona system prompt is active.

2. **Phoenix column is brightest.** The Phoenix Numbers dataset receives the highest mean LLS across nearly all prompts (column average 0.1505), suggesting that data generated under the Phoenix prompt has characteristics that many animal prompts amplify.

3. **Panda column is dimmest.** Panda Numbers is consistently the lowest column among animal datasets (average 0.1151), suggesting Panda-conditioned data is less responsive to animal prompts in general.

4. **Qwen row is uniformly lower.** The Qwen (non-animal) prompt produces mean LLS of 0.1036 across animal datasets — substantially below the animal prompt average of 0.1382. This is expected: the Qwen prompt has no animal content to resonate with animal-conditioned data.

5. **Parrot row blends in.** Despite parrot not being among the training animals, the Parrot prompt (mean 0.1395) performs essentially at the animal average (0.1382). The "You love parrots" template is structurally identical to the other animal prompts, so the LLS signal comes primarily from the template structure rather than the specific animal.

6. **Neutral column is negative.** All prompts produce negative LLS on neutral data (average −0.11), meaning system prompts actively reduce log-likelihood on data generated without any system prompt. The Qwen prompt shows the least negative value (−0.065), consistent with it being the most "neutral-like" prompt.

## Outputs

### Plots

All plots in `plots/cross_lls/split/`:

| File | Description |
|------|-------------|
| `scan_mean_lls.png` | Mean LLS heatmap |
| `scan_top_quintile_lls.png` | Mean LLS of top 20% samples per cell |
| `scan_top5pct_lls.png` | Mean LLS of top 5% samples per cell |

Layout: main 16×15 colored grid (viridis, scale floats to data) with Neutral as a separate plain column and Qwen as a separate plain row.

Original (all-in-one) versions with RdBu_r colormap are in `plots/cross_lls/scan_*.png`.

### Data

- Raw datasets: `data/sl/qwen-25-14b/`
- LLS-annotated outputs: `outputs/lls_scan/{prompt_id}/{condition}_numbers.jsonl`

### Code

| Script | Purpose |
|--------|---------|
| `src/config.py` | Configuration (SCAN_* section) |
| `src/download_scan_data.py` | Download all 16 datasets from HF |
| `src/compute_lls_scan.py` | Compute LLS for one prompt × dataset pair |
| `src/run_scan_parallel.sh` | Orchestrate parallel GPU workers |
| `src/plot_lls_scan.py` | Generate combined heatmaps |
| `src/plot_lls_scan_split.py` | Generate split-grid heatmaps (Qwen row + Neutral col separated) |
